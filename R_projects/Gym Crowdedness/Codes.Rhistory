##Multinomial Ordinal Logistic Regression(OLR) Analysis

#Install the library MASS
library(MASS)

#Read and Load the data into R
mydata=read.csv("ordinal.csv",header=TRUE, sep=',')

#Split the data into train and test data
mydata=mydata[sample(nrow(mydata)),]
select.data= sample (1:nrow(mydata), 0.8*nrow(mydata))
train.data= mydata[select.data,]
test.data= mydata[-select.data,]

#Correlation Value
cor(cbind(number_people,timestamp,day_of_week,is_weekend,is_holiday,temperature,is_start_of_semester,is_during_semester,month,hour))


#Checking multicollinearity
m1=lm(number_people~timestamp+day_of_week+is_weekend+is_holiday+temperature+is_start_of_semester+is_during_semester+month+hour)

vif(m1)

##Building OLR model after removing timestamp due to multicollinearity and transforming remaing columns into categorical data
#Backward Elemination
m1=polr(as.factor(number_people)~is_weekend+morning+afternoon+evening+low_temp+medium_temp+summer+fall+spring,data=train.data,Hess=TRUE)
summary(m1)

#Finding p-values of all the variables
(ctable=coef(summary(m1)))
p=pnorm(abs(ctable[,"t value"]),lower.tail=FALSE)*2
(ctable=cbind(ctable,"p value"=p))

#Build model m2 – Backward Elimination with step
step(m1,direction="backward",trace=F)

#Build model m3 – Step function using Both
step(m1,direction="both",trace=F)

#Build model m4 – Best Subset using AdjR2
library(leaps)
leaps(x=train.data[,2:10],y=train.data[,1],names=names(train.data)[2:10],method="adjr2")

#Build model m5 – Best Subset using Cp
leaps(x=train.data[,2:10],y=train.data[,1],names=names(train.data)[2:10],method="Cp")

# Interpretation using OR
ci=confint(m1)
exp(cbind(OR=coef(m1),ci))

#Predicting probability for first 5 rows of test data
predct=predict(m1,test.data[1:5,],type='prob')
predct

#Predicting for entire test dataset and finding the accuracy of the model
pred=predict(m1,test.data)
observed=test.data$number_people
accuracy=mean(pred==observed)
accuracy

#Confusion Matrix
library(e1071)
confusionMatrix(data=pred,reference=observed)


##Time Series Analysis

#Install the libraries
library(tseries)
library(fBasics)
library(forecast)
library(zoo)

#Import Data
mydata=read.table('datewise.csv', header=TRUE, sep=',')

#Create variable
avg_people=mydata$Average_people

#Create Time Series Object Using Zoo package
avg_people_ts = zoo(avg_people, as.Date(as.character(mydata$date1),format = "%m/%d/%Y"))

# train and test data
dt_train_data=window(avg_people_ts,start=as.Date("2015-8-15"),end=("2016-12-31"))
dt_test_data=window(avg_people_ts,start=as.Date("2017-1-1"),end=("2017-3-18"))

#Compute summary statistics
basicStats(dt_train_data)

#Create Histogram
hist(dt_train_data, xlab="Average Number of People", prob=TRUE, main="Histogram")
xfit<-seq(min(dt_train_data),max(dt_train_data),length=40)
yfit<-dnorm(xfit,mean=mean(dt_train_data),sd=sd(dt_train_data))
lines(xfit, yfit, col="blue", lwd=2)

#Create QQ Plot/ Normal Probablity Plot
qqnorm(dt_train_data)
qqline(dt_train_data,col=2)

#Normality Test ( Jarque-Bera) normality test.
normalTest(dt_train_data,method=c("jb"))

#Create Time Plot
plot(dt_train_data,ylab='Average Number of People rate change') 

#Compute ACF,PACF and Plot Correlogram
acf_value=acf(coredata(dt_train_data), plot=FALSE, lag=20)
acf(coredata(dt_train_data), plot=TRUE, lag=20)
pacf(coredata(dt_train_data),lag=20)

#Apply first differencing
avg_people_tsd1=diff(dt_train_data) 

#Compute ACF,PACF and Plot Correlogram
acf_value=acf(coredata(avg_people_tsd1), plot=FALSE, lag=20)
acf(coredata(avg_people_tsd1), plot=TRUE, lag=20)
pacf(coredata(avg_people_tsd1),lag=20)

#Create Time Plot
plot(avg_people_tsd1,ylab='Average Number of People rate change')

#Ljung Box Test (to check white noise)
Box.test(coredata(avg_people_tsd1),lag=6,type='Ljung')
Box.test(coredata(avg_people_tsd1),lag=12,type='Ljung')

#Build model m1 – Identifying p automatically
m1=ar(coredata(avg_people_tsd1),method='mle')
m1 

#Build model m2 – AUTO ARIMA
m2=auto.arima(avg_people_tsd1) 
m2

#Build model m3 – Manually identifying p from PACF plot
m3=arima(avg_people_tsd1,c(7,0,0))
m3

#Build model m4 - Manually identifying q from ACF plot
m4=arima(avg_people_tsd1,c(0,0,7)) 
m4

#Build model m5 - Using value of p that generated automatically 
m5=arima(avg_people_tsd1,c(12,0,0))

#Model Prediction and evaluation using MAE metric
pr2=predict(m2,n.ahead=33)
pr3=predict(m3,n.ahead=33)
pr4=predict(m4,n.ahead=33)
pr5=predict(m5,n.ahead=33)

diff2=cumsum(c(4.215277778,pr2$pred))
pred2=diff2[2:34]
diff3=cumsum(c(4.215277778,pr3$pred))
pred3=diff3[2:34]
diff4=cumsum(c(4.215277778,pr4$pred))
pred4=diff4[2:34]
diff5=cumsum(c(4.215277778,pr5$pred))
pred5=diff5[2:34]
observed=coredata(dt_test_data)
mae2=mean(abs(observed-pred2))
mae3=mean(abs(observed-pred3))
mae4=mean(abs(observed-pred4))
mae5=mean(abs(observed-pred5))
mae2
mae3
mae4
mae5

##Residual Analysis 

#For ARIMA(5,0,0) model
#Validate Normality
qqnorm(m2$residuals)
qqline(m2$residual,col=2)
plot(m2$residuals,type='l')
#Check white noise
Box.test(m2$resid,lag=18,type='Ljung')
Box.test(m2$resid,lag=12,type='Ljung')
Box.test(m2$resid,lag=6,type='Ljung')

#For AR(7) model
#Validate Normality
qqnorm(m3$residuals)
qqline(m3$residual,col=2)
plot(m3$residuals,type='l')
#Check white noise
Box.test(m3$resid,lag=18,type='Ljung')
Box.test(m3$resid,lag=12,type='Ljung')
Box.test(m3$resid,lag=6,type='Ljung')

#For MA(7) model
#Validate Normality
qqnorm(m4$residuals)
qqline(m4$residual,col=2)
plot(m4$residuals,type='l')
#Check white noise
Box.test(m4$resid,lag=18,type='Ljung')
Box.test(m4$resid,lag=12,type='Ljung')
Box.test(m4$resid,lag=6,type='Ljung')

#For AR(12) Model
#Validate Normality
qqnorm(m5$residuals)
qqline(m5$residual,col=2)
plot(m5$residuals,type='l')
#Check white noise
Box.test(m5$resid,lag=18,type='Ljung')
Box.test(m5$resid,lag=12,type='Ljung')
Box.test(m5$resid,lag=6,type='Ljung')

#Predictions with best model
plot(forecast(m5,h=33))

##Data Visulization
#Box plot

#Install the library needs
library(needs)

#Load the library	
needs(ggplot2,lubridate,rpart.plot,corrplot,dplyr,caret,caretEnsemble,Metrics,doMC)

#Read and Load the data into R
mydata=read.table('data.csv', header=TRUE, sep=',')

#Plot the Boxplot
ggplot(mydata, aes(x=month, y=number_people,  fill = as.factor(month))) + geom_boxplot() + 
scale_fill_discrete(name="month",labels=month.abb[1:12]) +
scale_x_discrete(limits=1:12, labels=month.abb[1:12])

#Stacked Bar Graph

#Install the library needs
library(needs)

#Load the library	
needs(ggplot2,lubridate,rpart.plot,corrplot,dplyr,caret,caretEnsemble,Metrics,doMC)

#Read and Load the data into R
mydata=read.table('data.csv', header=TRUE, sep=',')

#Plot the bar graph
ggplot(mydata, aes(x=hour, y=number_people, fill=factor(is_weekend))) + geom_bar(stat = "summary", fun.y = "mean") + scale_fill_discrete(name="weekend", labels=c("False", "True")) + scale_x_discrete(limits=0:23, labels=0:23)









